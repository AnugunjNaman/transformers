<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Omnivore

## Overview

The Omnivore Model was proposed in [Omnivore: A Single Model for Many Visual Modalities](https://arxiv.org/abs/2201.08377) by Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra.
The main idea here is generalization of vision transformer model across different modalities. The authors propose a single model 
for multiple classification tasks such as image classification, video classification and 3D scence classification.

The abstract from the paper is the following:

*Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, 
videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view
3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard
datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on
ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision
tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without
access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.*

Tips:

- All released checkpoints were pre-trained and fine-tuned on three datasets [ImageNet-1k]((https://huggingface.co/datasets/imagenet-1k)), [Kinetics](https://www.deepmind.com/open-source/kinetics) and [SUN RGB-D](https://rgbd.cs.princeton.edu/) respectively. 
  There are five checkpoints available are trained on said dataset. In single epoch training for the model it was trained on one epoch each for ImageNet-1k and Kinetics with ten epochs for SUN RGB-D.
- The authors of Omnivore released 5 trained Omnivore models, which you can directly plug into [`OmnivoreModel`] or [`OmnivoreForVisionClassification`]. 
  The 5 variants available are (all trained on images of size 224x224, video of 32 frames of size 224x224 and RGBD images of size 224x224):
  *facebook/omnivore-swinT*, *facebook/omnivore-swinS**, *facebook/omnivore-swinB*, *facebook/omnivore-swinB-in21k* and
  *facebook/omnivore-swinL-in21k*. 
- Note that one should use [`OmnivoreFeatureExtractor`] in order to prepare images, videos and RGBD images for the model.
- The input to the model needs to be provided with a corresponding type, i.e. *"images"*, *"videos"* or *"rgbd"* in order to use the classification head for that modality.

This model was contributed by [anugunj](https://huggingface.co/anugunj). The original code can be found [here](https://github.com/facebookresearch/omnivore).

## OmnivoreConfig

[[autodoc]] OmnivoreConfig

## OmnivoreFeatureExtractor

[[autodoc]] OmnivoreFeatureExtractor

## OmnivoreModel

[[autodoc]] OmnivoreModel
    - forward

## OmnivoreForVisionClassification

[[autodoc]] OmnivoreForVisionClassification
    - forward
